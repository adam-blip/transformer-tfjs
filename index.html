<!DOCTYPE html>
<html>
<head>
  <title>Optimized Transformer</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <!-- External Stylesheets -->
  <link rel="stylesheet" href="styles.css">
  
  <!-- Import latest TensorFlow.js -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.7.0/dist/tf.min.js"></script>
  
  <!-- Import GPT-3 tokenizer from CDN -->
  <script src="https://cdn.jsdelivr.net/npm/gpt-3-encoder@1.1.4/dist/index.min.js"></script>
  
  <!-- Import dataset -->
  <script src="data/crime-and-punishment_small.txt.js"></script>
  
  <!-- Modular JavaScript -->
  <script src="js/config.js"></script>
  <script src="js/utils.js"></script>
  <script src="js/layers.js"></script>
  <script src="js/tokenizer.js"></script>
  <script src="js/attention.js"></script>
  <script src="js/model.js"></script>
  <script src="js/generation.js"></script>
  <script src="js/training.js"></script>
  <script src="js/app.js"></script>
</head>
<body>
  <div class="container">
    <h1>
      <img src="logo.svg" alt="Transformer Model" height="40">
      transformer with tfjs
    </h1>
    
    <div class="controls">
      <button id="train-button" title="Start the model training process">Start Training</button>
      <button id="stop-button" title="Stop the current training run">Stop Training</button>
      <button id="download-button" title="Download the trained model for later use">Download Model</button>
    </div>
    
    <div class="progress" title="Training progress indicator">
      <div id="progress-bar" class="progress-bar"></div>
    </div>
    
    <div class="config">
      <div class="config-item">
        <label for="model-dim" title="Size of the embedding vectors">Model Dimension</label>
        <input type="number" id="model-dim" class="config-input" data-config="dModel" min="32" max="768">
      </div>
      <div class="config-item">
        <label for="num-heads" title="Number of attention heads for multi-head attention">Attention Heads</label>
        <input type="number" id="num-heads" class="config-input" data-config="nHeads" min="1" max="12">
      </div>
      <div class="config-item">
        <label for="num-blocks" title="Number of transformer layers">Transformer Blocks</label>
        <input type="number" id="num-blocks" class="config-input" data-config="blocks" min="1" max="24">
      </div>
      <div class="config-item">
        <label for="batch-size" title="Number of examples processed in each training step">Batch Size</label>
        <input type="number" id="batch-size" class="config-input" data-config="batchSize" min="1" max="32">
      </div>
      <div class="config-item">
        <label for="attn-type" title="Type of attention mechanism">Attention Type</label>
        <select id="attn-type" class="config-input" data-config="attentionType">
          <option value="linear" title="Linear complexity attention (Linformer-like)">Linear Attention</option>
          <option value="flash" title="Memory-efficient block-wise attention (Flash Attention-like)">Flash Attention</option>
          <option value="standard" title="Standard scaled dot-product attention">Standard Attention</option>
        </select>
      </div>
      <div class="config-item">
        <label for="norm-type" title="Layer normalization technique">Normalization</label>
        <select id="norm-type" class="config-input" data-config="normType">
          <option value="layerNorm" title="Standard layer normalization">Layer Norm</option>
          <option value="rmsnorm" title="Root Mean Square normalization (more efficient)">RMS Norm</option>
        </select>
      </div>
      <div class="config-item">
        <label for="activation-type" title="Activation function for feed-forward networks">Activation</label>
        <select id="activation-type" class="config-input" data-config="activationType">
          <option value="gelu" title="Gaussian Error Linear Unit (used in many modern transformers)">GELU</option>
          <option value="swish" title="Swish-Gated Linear Unit (better performance, used in PaLM)">SwiGLU</option>
          <option value="relu" title="Rectified Linear Unit (simpler, faster)">ReLU</option>
        </select>
      </div>
      <div class="config-item">
        <label for="learning-rate" title="Step size for gradient descent optimization">Learning Rate</label>
        <input type="number" id="learning-rate" class="config-input" data-config="learningRate" min="0.00001" max="0.01" step="0.00001">
      </div>
    </div>
    
    <div>
      <h3>Text Generation</h3>
      <div class="generation-controls">
        <input type="text" id="prompt-input" placeholder="Enter prompt text..." title="Seed text to start generation">
        <input type="number" id="length-input" placeholder="Length" value="100" title="Number of tokens to generate">
        <button id="generate-button" title="Generate text using the current model">Generate</button>
      </div>
    </div>
    
    <textarea id="log" readonly title="Training and generation log"></textarea>
    
    <div class="memory-info">
      <div title="Current memory usage by the model">Memory Usage: <span id="memory-usage">0 MB</span></div>
      <div title="Current number of tensors in memory">Tensors: <span id="tensor-count">0</span></div>
    </div>
  </div>
</body>
</html>